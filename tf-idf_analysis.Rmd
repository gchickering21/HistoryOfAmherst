---
title: "TF-DIF/Sentiment analysis"
author: "Clara Seo"
date: "10/02/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(readtext)
library(tidytext)
library(tidyverse)
library(tokenizers)
library(stopwords)
library(dplyr)
library(tidyr)
library(kableExtra)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

```{r data read-in}
# Read in all files from a folder called "chapter_files"
texts_df <- read.csv(file="all_texts.csv")
```

```{r}
n_chapters <- nrow(texts_df)
text <- c("")
book <- data.frame(text)

## This gets all the chapters into their own separate df's
for (i in 1:n_chapters) {
  # single string of text that contains the entire chapter
  temp_chapter <- texts_df[i, 3]
  string_chapter <- stringr::str_split(temp_chapter, "\n")[[1]]
  num_lines <- length(string_chapter)
  chapter_df <- tibble(line = 1:num_lines, text = string_chapter) %>% select(text)
  book <- rbind(book, chapter_df)
  # rename string with appropriate chapter number
  assign(paste("chapter", i - 1, "_df", sep = ""), chapter_df)
}
```


```{r}
ACbook_counts <- texts_df %>%
  mutate(chapterNum = X-1) %>%
  select(-X) %>% 
  
  #prepare data in useable format for text analysis
  tidytext::unnest_tokens(word, text) %>%
  
  # exclude stop words (i.e. the, an, a, you)
  anti_join(tidytext::get_stopwords(), by = "word") %>%
  
  # count word frequencies for a given chapter
  count(chapterNum, word, sort = TRUE) %>%
  mutate(freq=n) %>%
  select(chapterNum, word, freq)

tidy_DTM <- ACbook_counts %>%
  tidytext::bind_tf_idf(word, chapterNum, freq)

tf_order <- tidy_DTM %>%
  arrange(desc(tf))

tfidf_order <- tidy_DTM %>%
  arrange(desc(tf_idf)) 
```
Term frequency, tf(t,d) - raw count, number of times that term $t$ occurs in given document $d$, how frequently a word occurs in a document
Inverse document frequency, idf(t,D) - constant per corpus of D documents, accounts for ratio of documents that include the word $t$, measure of how much information the word provides (i.e. common or rar across all documents), logarithmically scaled inverse fraction, incorporated to diminish the weight of terms that occur very frequently in the given text and increases the weight of terms that occur rarely.
TF-IDF = tf(t,d)*idf(t,D), high tf-idf means high term frequency and low document frequency of term 

A central question in text analysis is how to quantify what a document is about. Can we do such quantification by looking at the words that make up the document? 

The tf-idf statistic has identified the kinds of words it is intended to, important words for individual documents within a collection of documents.
- given us insight into the content of NASA description fields
- good measure of the importance of a particular word for a given document 

Unlocking the past with historical account of Amherst College 

First, we looked at the most frequent terms. The usual suspects, unsurprisingly, are "stop words" such as "the", "are", "a", etc. After filtering the "stop words" we saw some potential patterns.

Using sentiment analysis, we noticed a genera sense of a cluster of particularly negative words in two chapters.

```{r}
# get_sentiments() - tidytext package's three sentiment lexicons
# bing lexicon categorizes words in a binary fashion into positive/negative
# afinn lexicon assigns words with a score between -5 (negative sentiment) and +5 (positive sentiment)
# nrc lexicon categorizes words in a binary fashion into categories of: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, trust
ACbook_sentiment_by_chapter <- tidy_DTM %>%
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(index=chapterNum, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

ggplot(data = ACbook_sentiment_by_chapter, aes(x=index, y=sentiment)) +
  geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) + 
  labs(x="Chapter", y="Overall Sentiment", 
       title="Sentiment Change Over the Course of the Book (bing lexicon)")
```

When looking at this graph, which displays the overall sentiment score by chapter we can see how the much the overall sentiment changes from chapter to chapter. One can see how chapters 14 and 26 are the only two chapters that have a negative sentiment score, whereas chapters 16,20, and 25 are chapters that have especially high sentiment score.

Note: heavily negative = Chapter 14 and Chapter 26 
Chapter 14 - Period of Reaction and Decline - Resignation of President Humphrey
Chapter 26 - The War

Stronger negative presence in two chapters. 

Note: most positive = Chapter 16, Chapter 20, and Chapter 25
Chapter 16 - Biographical Sketches of President Humphrey and Some of His Associates
Chapter 20 - The Presidency of Dr. Stearns
Chapter 25 - Benefactors of the College


```{r}

nrc_word_counts <- tidy_DTM %>%
  inner_join(get_sentiments("nrc"), by = "word") %>% 
  count(word, sentiment) %>% 
  mutate(method = "NRC")

afinn_word_counts <- tidy_DTM %>%
  inner_join(get_sentiments("afinn"), by = "word") %>% 
  count(word, value) %>% 
  mutate(method = "afinn")

bing_word_counts <- tidy_DTM %>%
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(word, sentiment) %>% 
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(20) %>%
  ggplot(aes(reorder(word, n), n, fill = sentiment)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment", x = NULL) +
  coord_flip()
```


```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ggplot(aes(reorder(word, n), n, fill = sentiment)) +
    geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
    facet_wrap(~sentiment, scales = "free_y") +
    labs(y = "Contribution to Sentiment", x = NULL, 
         title = "Top 10 Contributors of Each Sentiment") +
    coord_flip()
```

```{r}
nrc<-tidytext::get_sentiments("nrc")

AC_sentiment_by_chapter <- tidy_DTM %>%
  inner_join(get_sentiments("nrc"), by = "word") %>% 
  count(index=chapterNum, sentiment) %>% 
  spread(sentiment, n, fill = 0)
AC_sentiment_by_chapter

```

By breaking down the sentiments not just into negative and postive but rather getting a sense of what emotions are actually being highly used in the chapters, one can begin to get a better understanding of how the material in that chapter is going to be written about.

```{r}
ggplot(data= AC_sentiment_by_chapter , aes(x=index, y= fear)) +
  geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) + 
  labs(x="Chapter", y=paste( "Fear Count",sep=" "), 
       title=paste( "Amount of Fear Change Over the Course of the Book (nrc lexicon)", sep=" "))
```

From this graph we can see that the individual chapters range widely in terms of the amount of fear contained in the chapter. As we can see chapters 0,3,15, and 27 are all chapters with very low fear, whereas chapters 14, 16, and 28 all have very high levels of fear. 
Note: Chapters with High Levels of Fear
Chapter 14 - Period of Reaction and Decline - Resignation of President Humphrey
Chapter 16 - Biographical Sketches of President Humprey and some of his associates
Chapter 28 - Then and Now - Panoramic Review of Change and Progress

Note: Chapters with Low Levels of Fear
Chapter 0: PREFACE
Chapter 3: AMHERST ACADEMY,
Chapter 15: THE RELIGIOUS HISTORY OF THIS PERIOD
Chapter 27: THE SEMI-CENTENNIAL CELEBRATION

```{r}
ggplot(data= AC_sentiment_by_chapter , aes(x=index, y= joy)) +
  geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) + 
  labs(x="Chapter", y=paste( "Joy Count",sep=" "), 
       title=paste( "Amount of Joy Change Over the Course of the Book (nrc lexicon)", sep=" "))
```

When looking at this chart, it highlights that overall this book always expressed high levels of joy, at least when we compare it to the previous graph Other than Chapter 0, which is the preface, although there are chapters that express especially a high level of joy, that even the lowest chapters have high levels of joy. This can give us a sense of how the author of the authors true feeling about the college and the viewpoint in which the writing is going to be from. 

---
title: "TF-DIF/Sentiment analysis"
author: "Clara Seo"
date: "10/02/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(readtext)
library(tidytext)
library(tidyverse)
library(tokenizers)
library(stopwords)
library(dplyr)
library(tidyr)
library(kableExtra)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

```{r data read-in}
# Read in all files from a folder called "chapter_files"
texts_df <- read.csv(file="all_texts.csv")
```

```{r}
n_chapters <- nrow(texts_df)
text <- c("")
book <- data.frame(text)

## This gets all the chapters into their own separate df's
for (i in 1:n_chapters) {
  # single string of text that contains the entire chapter
  temp_chapter <- texts_df[i, 3]
  string_chapter <- stringr::str_split(temp_chapter, "\n")[[1]]
  num_lines <- length(string_chapter)
  chapter_df <- tibble(line = 1:num_lines, text = string_chapter) %>% select(text)
  book <- rbind(book, chapter_df)
  # rename string with appropriate chapter number
  assign(paste("chapter", i - 1, "_df", sep = ""), chapter_df)
}
```

```{r}
ACbook_counts <- texts_df %>%
  mutate(chapterNum = X-1) %>%
  select(-X) %>% 
  
  #prepare data in useable format for text analysis
  tidytext::unnest_tokens(word, text) %>%
  
  # exclude stop words (i.e. the, an, a, you)
  anti_join(tidytext::get_stopwords(), by = "word") %>%
  
  # count word frequencies for a given chapter
  count(chapterNum, word, sort = TRUE) %>%
  mutate(freq=n) %>%
  select(chapterNum, word, freq)

tidy_DTM <- ACbook_counts %>%
  tidytext::bind_tf_idf(word, chapterNum, freq)

tf_order <- tidy_DTM %>%
  arrange(desc(tf))

tfidf_order <- tidy_DTM %>%
  arrange(desc(tf_idf)) 
```
Term frequency, tf(t,d) - raw count, number of times that term $t$ occurs in given document $d$, how frequently a word occurs in a document
Inverse document frequency, idf(t,D) - constant per corpus of D documents, accounts for ratio of documents that include the word $t$, measure of how much information the word provides (i.e. common or rar across all documents), logarithmically scaled inverse fraction, incorporated to diminish the weight of terms that occur very frequently in the given text and increases the weight of terms that occur rarely.
TF-IDF = tf(t,d)*idf(t,D), high tf-idf means high term frequency and low document frequency of term 

A central question in text analysis is how to quantify what a document is about. Can we do such quantification by looking at the words that make up the document? 

The tf-idf statistic has identified the kinds of words it is intended to, important words for individual documents within a collection of documents.
- given us insight into the content of NASA description fields
- good measure of the importance of a particular word for a given document 

Unlocking the past with historical account of Amherst College 

First, we looked at the most frequent terms. The usual suspects, unsurprisingly, are "stop words" such as "the", "are", "a", etc. After filtering the "stop words" we saw some potential patterns.

Using sentiment analysis, we noticed a genera sense of. a cluster of particularly negative words in two chapters.

```{r sentiment}
# get_sentiments() - tidytext package's three sentiment lexicons
# bing lexicon categorizes words in a binary fashion into positive/negative
# afinn lexicon assigns words with a score between -5 (negative sentiment) and +5 (positive sentiment)
# nrc lexicon categorizes words in a binary fashion into categories of: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, trust

ACbook_sentiment_by_chapter <- tidy_DTM %>%
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(index=chapterNum, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

ggplot(data = ACbook_sentiment_by_chapter, aes(x=index, y=sentiment)) +
  geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) + 
  labs(x="Chapter", y="Overall Sentiment", 
       title="Sentiment Change Over the Course of the Book (bing lexicon)")

nrc_word_counts <- tidy_DTM %>%
  inner_join(get_sentiments("nrc"), by = "word") %>% 
  count(word, sentiment) %>% 
  mutate(method = "NRC")

afinn_word_counts <- tidy_DTM %>%
  inner_join(get_sentiments("afinn"), by = "word") %>% 
  count(word, value) %>% 
  mutate(method = "afinn")

bing_word_counts <- tidy_DTM %>%
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(word, sentiment) %>% 
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(20) %>%
  ggplot(aes(reorder(word, n), n, fill = sentiment)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment", x = NULL) +
  coord_flip()

# identify and visually assess top 10 contributors to each sentiment
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ggplot(aes(reorder(word, n), n, fill = sentiment)) +
    geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
    facet_wrap(~sentiment, scales = "free_y") +
    labs(y = "Contribution to Sentiment", x = NULL, 
         title = "Top 10 Contributors of Each Sentiment") +
    coord_flip()
```
Note: heavily negative = Chapter 14 and Chapter 26 
Chapter 14 - Period of Reaction and Decline - Resignation of President Humphrey
Chapter 26 - The War

Stronger negative presence in two chapters. 

Note: most positive = Chapter 16, Chapter 20, and Chapter 25
Chapter 16 - Biographical Sketches of President Humphrey and Some of His Associates
Chapter 20 - The Presidency of Dr. Stearns
Chapter 25 - Benefactors of the College

https://uc-r.github.io/sentiment_analysis